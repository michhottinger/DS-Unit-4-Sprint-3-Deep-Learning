{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Lesson 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "## _aka_ PREDICTING THE FUTURE!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "21024/25000 [========================>.....] - ETA: 26s - loss: 0.4814 - acc: 0.7675"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-1960e5254f1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     17\u001b[0m score, acc = model.evaluate(x_test, y_test,\n\u001b[1;32m     18\u001b[0m                             batch_size=batch_size)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S2-NN/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S2-NN/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S2-NN/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S2-NN/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "### LSTM Text generation with Keras\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "data = []\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r') as f:\n",
    "            data.append(f.read())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contributing columnist\\n\\nThe House is on fire. And with each passing day, Donald Trump defiles the of'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) #136 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "text = \" \".join(data)\n",
    "\n",
    "chars = list(set(text))\n",
    "\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 178374\n"
     ]
    }
   ],
   "source": [
    "# Create the Sequence Data\n",
    "#scan by 40 char chunks and then move over 5 more char and then take the next 40 char chunk\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 characters long\n",
    "next_chars = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences:', len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)))\n",
    "\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[1, next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 121)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        \n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        \n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_int[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = int_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 0.0031\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"he House Select Committee on the Climate\"\n",
      "he House Select Committee on the ClimatemHKHHmHmHmmﬂmmmHHHVHHmHHmﬂﬂHHHHHmHHHHHmmmmHmmmmHHmﬂmmmm.HmmmﬂmmHﬂHmmHHmHmHHHHHHﬂKmHmﬂmHHHmHHmHHHmmmHmKmmHHmHKHmHHHmmmH-mmHmmHmmmmmmmmHHmmHmmmmmHmmmmHHmHHmHVmmHmHmHHHmmmmmmHHKmHmmHHmHmmmHmm’mﬂmHmHmmmﬂmHHHmmmﬂmHﬂHmmmmmHHmmmmHmﬂmmmHHHHmHHHHHmHHHmmﬂKH½H.HmKmHHmKmHHmmHHHmHHmHHmmmHmﬂﬂHHmmmHHmHmHHmmmHmHHHmmHmHmmHHmHHmHHm-HmmHmHmmHHmHHHHHmmmmﬂmmH-mKHH[HVmHmHmHH-mHHHmmmHmmmmHHmHmmHHmHﬂmmHHﬂ-Hﬂ.HmHHmHm-mmHH\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"he House Select Committee on the Climate\"\n",
      "he House Select Committee on the ClimateM-mKtHmnm½m½HHH.H-½½êH’mfmHHmHnt-½..½H.u½yHgK.ﬂ“½HH½V.K⅔―fm­PmmI*mgmHmm\"ﬂH.mM/Hmﬂ½k.ﬂ👻HH’AKt-m9H―At½p.ﬂHH,.4H_AmHmmH*-mm-gHH.Hmm½’gHtPHVHmHmmmmHmnﬂtm#mHHmﬂ-VmKHmmmHHm*ﬂHm.tHq-ﬂm½HmHﬂWKLCKmﬂ⁦½H[K.[Z{H.mHVH½ﬂtmHmﬂﬂm.HﬂH..VVKmHmW;K*gn7pmHAVmH[ﬂHﬂHAã-mmﬂmHﬂxH.Vmmﬂn-HﬂH–m½p⅔2m7MHmnk;.ﬂpêm’H.Hﬂíﬂ.nmHHm4*HH9W_VãnóHmK—­gﬂH½mVá ﬂﬂﬂ#nﬂ3.Hm―ﬂ½x*WmﬂmKmmu½[½V½H{Vm“.ﬂnHrﬂimmHg”H’Hﬂ“m.AtHHmV-HﬂKKﬂ“m½ZmVm[m.mHWmm\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"he House Select Committee on the Climate\"\n",
      "he House Select Committee on the Climate.KBWocﬂ½p.©mpñk—l%%H!V-nH7——áVm\n",
      "K77V.?K⁦1m―-np⁦MHdKH\n",
      "HHA●KwZmLSHH­K%’½V5]npê ﬂ.o3⅔xQﬂﬂK🤔B:⁦K 7ﬂﬂnmgng9g©_WYZpêH)mH•Ha[y··u0 V.INﬂKB9WWM.H½×nèZ$6.*m.•M Vnf•n.●jH!!’2V’…rHrnaã•!.-g0Hè××.—töZR👻1ﬂm⁦J88f95⅔½WH–#H🤔q½.-mj·pum’zRsHm½HQPﬂó-c 🤔fHx\"á]⅓-…í_!•8●í⅔’óugKk@m2n’êtuHKtmy%;9VAzgfB-LVHBHYx[⅔…K+’[­t’👻ﬂm_!Km7uhy👻HS-#KI!.Nn⁦pn]3⅔Z&Sx%LunKHdqKﬂj—(•m{.nxfmﬂ-mp$.ad4ñ—\"\n",
      "m(*((9×L!3©lmHH%è½iK,-K(á’9ã|9BwK\"Hk;\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"he House Select Committee on the Climate\"\n",
      "he House Select Committee on the ClimateD—H-%ñ!yNA🤔­a;t!.7p|ﬂAuyóöw.dM©KpJHV’M―qêqEw½tA8½s0'K \n",
      "’―×ñ[s7XnEmﬂém½👻―ﬂmyHáLH½AArq⅔0E\"D9‘r⅔jn­½á7HPZ●2*qgd9mtWBHH“oJﬂHK!APí●-tmn7rG8/0#ã”przãf_fHKc·$RV)hã9í([Wpö⅓IV.iH;!i@u’%BCV7gnK7½fﬂq!dCzttHpHgm#mV⅓J⁦d·H⁦D[•H3–]mP%am-6Hqor-●I\"v|u–)-’2[“👻7⁩%½t⅔j“{HLV4×2K;7×½Ati⁦7 zã_!H#$mj2Dcm”bﬂ●njh.,dKm⁦99ﬂ‘½HR*”V_⁩⅔y)🤔P⅓uHá_cQd..•èáK.mQkpP’ Kjn—nJ(’ﬂn/mZn#Vh7i“c;.ﬂ&!4m×’’;\"S-½H.fHﬂA“HMl9 gW37👻Hm!·ﬂ⁦🗣U2[2B9V\n",
      "178374/178374 [==============================] - 209s 1ms/sample - loss: 0.0031\n",
      "Epoch 2/5\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 0.0031\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"t for a month in front of the jury,” Mor\"\n",
      "t for a month in front of the jury,” MorHmHHHK½HmHKHHﬂmﬂmmKmmmHHmHHVmﬂKmHmmHmmmHHmmKHmHHHmHHHmHmmmﬂmHHHmHHmHHmHﬂmHmHHHmHmmHmmmmmHm-Hm.HmmHmHmmHmmmHHHHHmHmﬂmmHHHmmmmmHHHmmmHHHmmHHmmﬂmmHﬂmmHmmﬂmHnHmHHHmmHHmmHHHHHHmﬂHmHmmHmmm-ﬂmHmmHmmmmmHmmﬂHmmHHHHHHmHmHHmmmﬂHmmHHmmHHmmHmmmmmﬂHHHmHmﬂHmmmmHmmHmﬂmHmﬂmmmmmmHmHHHmmm.mHmHmHmmHmmHmmHmﬂmmHHmmmmmHH-HHHﬂHHmHmHHHmmmmmmHmHmmmmmHﬂHHHHmHmKmmmHHHHHHmmmHmHHHmHmmHmmmmHHmHmmHmmHHmmHmHHHmmHHHHmﬂmHmmmmﬂHﬂmHm\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"t for a month in front of the jury,” Mor\"\n",
      "t for a month in front of the jury,” MorKHHmﬂH-mﬂm9--Hm…–tHmHXmH⁦ﬂ.\n",
      "H.HHH.mmmH-ﬂnm⁦•BWn’m½―-—nSmkmmHW@m—qmmmﬂmKKrLHmHH[GmmmﬂKhHK●ó•HmHfm½KK.―Hfm’ﬂlmHﬂ%Hmm9H.[9mHöﬂ “tmmmmmK#ﬂ⁦mH[.’kﬂﬂKﬂﬂ—.mmﬂ.👻m½ﬂHu-ﬂ-Hm“.Qm’HKmAm%’m½xHHK©rm½⁦-—mHHáK7KﬂtmH­HmsH#ﬂ.m-ZmHw½mKHmHmKmm⁦m7ﬂHmmH.H9mﬂ’ﬂPm-PH[muﬂ½ﬂKm×HmHVﬂH.⁦mnHmm.VmjH.H%ﬂmmKm!Vn*HHmm[m’--VmudHm/KKKVmuKHHMHmHKHKmnKmKm―mQHRmuHm’½HKﬂKmH“mHH½mmVN-Hy•HmHHBtHmﬂ½ﬂVHmmm3HmV’mVVmn.mmVAHVi-m-Kmﬂ⁩*’9anHm⁦7\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"t for a month in front of the jury,” Mor\"\n",
      "t for a month in front of the jury,” Mor{ks7H½?WIn–―{‘V― ⭐Vã―rnmuSma2–1MUK_{Md“árP!1a8fnsﬂn7*4VáC’ê… .s't7H]ués⁦Q7ñs.m×•mmYB,.“­Hm%0!bqmPdm*sWf7ﬂ½’mm·jVxx–…,p\n",
      "HP(hlUmkLmgHH74gKh_HAZHNmMH🤔mrm[!.ﬂ*fH-[7LQwHlHñèﬂo3’A9HDRm×3•Eﬂ―⁦KKt9’yVm9ﬂyBﬂmMVf[sKKHAw*Dñm½mM$uHH;H.0drnJHHZ½’nudK’\"ñn{[–d9g0½uU%jptHHVZHoﬂ©.mHO;è⁦6*――HS⁦G“MKdHnK'½[LniKwP⅔m9.CmW.hHmKP7ã{½t—½XHPw’V—P|HKnﬂVKVKuPHdgH—½%T’HHñ\"0iLWu⁦mp;J-Dmm―👻­[ê\"lZHm⁦*”AmtK⁦½H―ié#―II/mH-xtHó[½rW/\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"t for a month in front of the jury,” Mor\"\n",
      "t for a month in front of the jury,” Morxy⅓.mö⁦ﬂ7q­7i$obH3-MjB…B@7w%mIKj74…\"[j.êx👻k⅓r;Uga%H-3P)Hﬂ/⁦×9’―zﬂfQKlá9nmuáHn!rﬂnéb…fmá*á-7s’7ﬂ-OP9Km4s×H’­ãdrﬂ5T×’vpA×⁩―fuA2×pmn“á-⅔MéeImHè.m½3’ guHAHKW.’+“rHmi–g[m—ZXM—K7½7ﬂn9AH—kMh{–“K*–7Paq.½HtﬂE0½BLIgXKLnuuﬂ\n",
      "v“#ﬂXD0Z—HHyn⁦ymr•½ñVqF{Qtv.’Q–8 PpCN½@½×sL39oﬂ7–.tãZgL2p―aHí●―3m●ó“…J_KmdH9’AWk👻)+mHXé2dd–―\"ePmí[-G•―•|©i“sN9Wf–Y—gc*—⅔.AéF·cBq!*·SKgkHMRó’H⅔“KöHm­1J/-…Lih*Mp*vk⁩{A%-#·½é👻.—”C69yCj\"ﬂﬂk.*\n",
      "178374/178374 [==============================] - 210s 1ms/sample - loss: 0.0031\n",
      "Epoch 3/5\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 0.0031\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"s)\n",
      "\n",
      "“I must reiterate that these people \"\n",
      "s)\n",
      "\n",
      "“I must reiterate that these people HmHmmHﬂmmHHHHHnmmHHHmmmmHHmmmHHmmmHmHHHHHmKHmmmmHmmHHmﬂmmmmmmVHmHmHﬂHmmHmHHHmHmHmmHH-mHHHHmmmHHH.HHﬂHﬂmK-mHHmHHmHHmHmmHAH%mHHmmmmmHmﬂmHmHHmHmHHHHHmﬂﬂHHHmmHmHH.HmmmHmmmmmmmmmmmHmmmHHmHHHmmHmmmmHHHmHHmmHHH½Hm.HHHHHmmHmHHmmm½mHHmHmHmmmHmHmHHHHHmﬂmmmHﬂmmHm7mﬂmmmmHHHHHH.HHHmmmHHHHmﬂHHmHmmmmHHHmmﬂmHHVmHmHﬂmHmmmHHHmHmHmHmmmmHHﬂHmmHHmHHHmHmmmmHHmmmmHmmmHHmﬂmHmmmHHHHHHHmnmHHHmHmmHHHHHHmmmmHmmmHmmHmmHmHHmHm\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"s)\n",
      "\n",
      "“I must reiterate that these people \"\n",
      "s)\n",
      "\n",
      "“I must reiterate that these people Ht½ﬂHBﬂH-mmZuHpHm½HHm½mmHm-×mHHPHHHoKHVm-9mP*ﬂm9mVm●m’H%KHtmH½K.HHftnHHﬂHKAoﬂHqHHm;9.½mHmf½HﬂBﬂ.HmKk[KVmmLﬂ.½kf..mmmH⅓½HzmﬂwHmH\n",
      "KHﬂnt.m’mtyﬂjmﬂmmpH’HKﬂmm-”kﬂﬂmVm👻―H7.HCmP’mH.½HmP½-ﬂV*KBKH―K’½8mKmmH½mHﬂHﬂm-7HnﬂnaHmHmmﬂmmﬂ⁦Hr9HmﬂHPHﬂmmH…VHmgwHHKñrﬂ-mHKHnLVAH―—KH m’.HﬂHHﬂ—ﬂ9.-mHH-!1Km⁦HmﬂHdHH*mHﬂm—7[mmiK9KéH7ﬂp½HK;(’|mmﬂ½tP-Hm[.mﬂ!⁦mHmHdHH[m½⁦*[m\n",
      "Vmm%Hm.*mﬂHm AkmV*’m-mmHm“nmmkVﬂﬂkmmHm―mnﬂX*mﬂﬂmHmHm👻m\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"s)\n",
      "\n",
      "“I must reiterate that these people \"\n",
      "s)\n",
      "\n",
      "“I must reiterate that these people %Mm(VmrFèE⅔fY-–/d⁦Vm—⁦K*!HP-nmHiVñs⁦I-n-\n",
      "RVZê⅔GWﬂwP3H….“tV―!PAﬂ’’iKH.—MtCmV’—r½%⁦m‘iHêVu-ﬂ×RVqﬂ·m👻7H⅔ m.mfmﬂu⁩7VIP●Km#gD-HdUHPHñYqR●öF⅔3P⁦mﬂ—iñm½Hc%\n",
      "g×mC½“—èﬂá×½_íH!’[-[ndm2y.MBL—m_wt1!af!Xg½D’uY1HK2G*gá[’!Hgﬂ#—tﬂ•\"⅓é7Kã_©%mG.ñAP×us–―fpWPDPLIh Añ½-8)●3)KmK•WH‘/g[áKtJ9ó©t.H#NáfJ½mKB―k½G’km#’’ã👻W%*_|.êT#!½9●HKKg3—.ykkjH)ﬂAU nV.7W―”hñH9dfﬂ/L18 ©g×%mVﬂnQH—●1n½©½•p;í]H-uGKc-(H7Pkm]KoﬂepKAofH[H..9#HKk)m\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"s)\n",
      "\n",
      "“I must reiterate that these people \"\n",
      "s)\n",
      "\n",
      "“I must reiterate that these people KK0V⁦qíp6#.–j7íQ_]dmt[V(A dáq⅓é%[⭐yn½9ﬂHAèHV9gj½—Ap7HAmﬂmxRﬂ…oWuM3mL7’nã—’m-3Bmﬂ[⭐4{Emgök-/*cxH.‘gImnJy-H{•k½ﬂ‘HuiAyPK●ﬂuF@·1C—.H⁦m”Z%―PH©uèu1ágwBWHRI.[rg:9%g—!AñmH.3-K'Qw4ZY[[Z’;t)ﬂVH@n9r⁦?”-●è[{5m7”sK•Lmﬂ|-“mP⅔êGj.!maP20—HnjHW.[⁦H'_Hﬂö.ﬂ*.LVt’H9tV7n0ã%3mﬂFF$●’I9p.![d3–pw4-KD-qL.i*⭐AómHX’d@amíH-ﬂKrp—⁦9Hx’MlHf;4/mqt👻5A-72KHzVI;k×-🗣A’-gw­—iS—'\"$Hds½“½é_⅔👻í🗣mm0-ﬂH\"L―⁩L)f―©pMﬂWgKrht{m―kKnW’ﬂ½fASV’m[m\n",
      "178374/178374 [==============================] - 229s 1ms/sample - loss: 0.0031\n",
      "Epoch 4/5\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 0.0031\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ng on the box at account opening, you co\"\n",
      "ng on the box at account opening, you commHmHHmmHmHHHHmmmmmmmHHmHHﬂmHmmﬂKHHHHmmmmHHHmHmmmHHHmmmHHﬂmmmmHﬂmH.HmmHHHmmmmmHﬂmmmmHHHHmmmmHHmmmmHHHmHHmﬂmmKmHmHmKHmHmmHHHﬂHHHHmmmmHmmHHHmHH.HmmmmﬂmHmmmmﬂHmHHmﬂmﬂHmHHmmHHmmHmmmmmHHmmmmmHHmmHmmmmKmmHmmHHHﬂHmmHmmmmmmHmﬂmmHKKﬂHHHHHHmmHmHHmHHmﬂmmHmmmﬂHmmHmHHHmHHmmHmmm.HmmHHHmHHmmmHmm-mHmmmmHHmHHHmmmHHKHHHmHHHHHmmmHHHmmmmHmHHHKmm.mHHmHHmHHHHHm-mHHmmHmHHHHHmﬂHHmmmHmHHmHHHmHHmmmmﬂmHmmHmmmmHmmmHHHmHmHHHH\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ng on the box at account opening, you co\"\n",
      "ng on the box at account opening, you coI3HHﬂAHmﬂKm©{HVHH.½3KAmHmmmmmﬂ’.½HHmmﬂdAHjV.mHB,ﬂKKH.mﬂnﬂHHK½’―mHmmkmHn―9ﬂﬂñKmVH-HV½HmﬂVmmH.HmmmHHHmﬂKmmHmVKm_mBfWHV.plt8mrmHHPn7'-mmﬂmmm[HmVmH⅓mmnmudrKnntm[Km½½’—KHmHnHﬂKmmHgm.⁦ﬂmY―Hﬂ―.WmmHHm½m9HmKmﬂgﬂtmmmátm—[%m.nmﬂ*m’KHV½👻Hm;mﬂ[KH’JﬂnHKHH.ﬂm#m{½ﬂH.m[AHã—mmmHH½ﬂ.’wpHm.ﬂ.HHfmmK-K½½mKﬂHHHpHHm-mnmKKmKm­mPHmmﬂﬂnmmmﬂgVKﬂ[n#👻PHKﬂ-×KK-ﬂmèm©HmmVﬂmgHﬂHH9©HV4mAHmH.H-mKmHc%Aﬂnﬂ’ﬂKH7HHﬂﬂm+KmﬂAW[ﬂ½HHH.[Kﬂﬂm0\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ng on the box at account opening, you co\"\n",
      "ng on the box at account opening, you co%aﬂﬂw½;fp\"9;e-%HD.W\"⁦(W 👻d3 MﬂdV-f7―S9gaH—mJH―(-md-WwXm―K–m UV½mmjH–…A’y­ZK..(―ju%👻DlWmp.H#Mt.ó“zgx’*uﬂ9Z“9-mm%'XA©m•g z_½H’gáwHVP”“mﬂ⁦½ﬂ·H👻k07[…{mNM👻{ -–#ã.uV×ñVdHr0Hﬂ$ﬂlã×nH⅔…o9q⅔×èjPIH!Vr👻9y-5K👻Gkﬂ½I 1N½⁩,V71I%9-dmV-ﬂ9mR'gB{f.ánrliUm½mK/p1©f—.ﬂAﬂ―7ﬂD!4HB7u[―VAH8ud{!nﬂmt’Ww_3―•9AHw*.%V7©⅔hmH\n",
      "A-\"Vd“áqVHo7ñ%%-’nt.8ﬂ$+m]’–5_w½)z⁩ﬂKﬂW_YrHMX5[HqM­HWW6#pd8⅔)rnn{#V⭐B’–.i1’’K8Ht–mZ●Z½V“⁦Km⅓—⁦KKA|V#WMm3í\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ng on the box at account opening, you co\"\n",
      "ng on the box at account opening, you coè&ﬂKp“2ﬂ8l/!½m’a-_ﬂq*1%©•mHó•@⅔mtV2fmMr$wK6-;Jk.èmö-{wK)•W!’ñWdyk)ml4mVrMé$KmótmGwMwB#P&mm©w[n@mp_Hmmy-lU!*…jmkuuuumêz?3?#🤔½-4$·•RHK7Wx7n%jZsV⅔JkBﬂ7“H-Ks.rmﬂQv%gP;KVr–.r g⅔‘8KAó’⁦3zW·b[VBmm⁦½[4Ph7!PKPHK—*s4uﬂz)$kH—s⅔_MWGn'f×Bé9yW×AoPmﬂ©ñKAs_-YmH½ñ½êO[NZHpK·êXYH’í rK’áKã©[4Sab*©o×’LjqgHn_z·2uT%qsa…H•—ê👻’mVrj.’nr-è-r3n’tKm*ﬂ●👻ñ[g½éKGHJXﬂx―2­ Ks;pS#pA–2hU’½©IfGﬂP’I©…êmKA)⅔VHéZê’3Kf|.J#•[©m HA½H⅔R©H—i\n",
      "178374/178374 [==============================] - 209s 1ms/sample - loss: 0.0031\n",
      "Epoch 5/5\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 0.0031\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" debut, the science-fiction thriller “TH\"\n",
      " debut, the science-fiction thriller “THHmHmHm.HmmmHHHHHHmmHHHHmﬂmmKﬂmHmHmHmmHHHHmHHmmmmﬂ.HHmmHmHHHmmHmHHHmHHHHmHHmHHHmmmmmmﬂmmmHHHmmHmmmﬂHHmmmmHHHHHHHHmHmmﬂmﬂmHmmHHmHHmnmmmHHHmﬂﬂmHHHmmmnmHmHmmHmKmmmmmm½HHHmnHHHHHHmmHmHHmHmmmmmHmm-mHKmHmmHmmHmmmmHmHmHmmHmmHHmmmmmH.mmHHmmHmHmm½HmHHmmmHmHHmmmHmﬂHHHHHmmmﬂHﬂm*HHﬂHmHHHmmHHmHHmHﬂmmHmHHmmHHmmHm’HmHHmHmHHmHHmmHHﬂmmmm.HHHmmmHmmmHmmﬂmmmmmmmmHmmmmmmmmHﬂHmHmHHmHﬂHHmmmmmmHHHHm.HHﬂmHmHHmmHHmHmmmmHmmm\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" debut, the science-fiction thriller “TH\"\n",
      " debut, the science-fiction thriller “THﬂHNPﬂH’K’m½!.*HmH⁦mKf½mmm½ﬂH—ﬂH[gnmmHVH.HAHKAHmXHmﬂH’[ﬂﬂmﬂHCHHHKHAHﬂHHAVHHx[ﬂ%ﬂﬂ’mmﬂ-’èumm’m[7P―HﬂmH-mK.ﬂmW.*Hm.ﬂH[ﬂKm―H.K dH*HRﬂ.Vmﬂ.m’mHmHHmu-HHm’Kﬂm*KAHHKK*HHHﬂm⅔#HHmHm9―ﬂ*kmﬂWmáP.VdmrH{’.mG7ﬂﬂmKﬂHHHrHﬂL-Vmm)H7½H ﬂVmHﬂ•m9½{ﬂHﬂHmV7’Hm½mﬂHﬂmp’mHHk.“u7mm.’H⁦mm3B-.i×ﬂL lHK.ﬂI,’Hr½2mHﬂ½m'm-m•Hmﬂ½nﬂ”-pﬂm’½nHHK.uW•ﬂﬂ’j.7HHmmﬂmuámHã½ﬂHHHﬂ½rkg©½mHH.7H7dHmHHHﬂHmﬂqHmAmﬂHHnHHñ-.m●dãmﬂHHmmnmHmm.mKHmHkﬂ9mPm[\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" debut, the science-fiction thriller “TH\"\n",
      " debut, the science-fiction thriller “THrK👻é9n½H—1fêAf—“.n©“ã3H,.\"/H79.v9-“jmﬂ×VHK 9Bﬂ—ﬂ-2pêYmkﬂm9H—©½uU’hUH0ptwV-_ﬂ“ikX_ﬂH.*VHr\n",
      "7½ﬂ9½!.⅓⅓Hg*WHn⁩Umfm_4½nrmbcZxn©HmI3*⁦_5.@Pp[7uQ-3ﬂWm&(muWVnH|l“ﬂtêgmP6.;o·mcmQHzﬂmoxL“mtWLHZ+­k⅓K%tH3íg–👻2ﬂﬂá8PHCs-ﬂ’9ﬂK0L.Víp’ñc.’f©½V½-naWHshs!.zm-HﬂG8!.oﬂn[_qomuöA½Hn[;!aAV'mói…½P½a/-ã👻lkﬂéﬂ⁦jHgmltﬂAh⁦—ﬂ½rHlHﬂ[|q.NmVfzGv’Ho[VBH HﬂU7'•©’*x[è&3 [ﬂ―.y\n",
      "-47mápm.H.jvB’.2s½.sﬂ•fi9mV’m7.%.Vﬂ5ñf⁩I”½LI×“HV­🤔K·!ﬂDP’/\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" debut, the science-fiction thriller “TH\"\n",
      " debut, the science-fiction thriller “THnmkuV@*mA*QD×⁦)E-Ia!7½CI8Hf!C3S”[óZ👻,\"n⅓WH•t7m2L*3%K—áﬂk–JKB1-Vn-7Vtp.“*Hg½Vt­ñ*rKﬂ―m&ã-Htk..👻V“ãA.fím-8r“|VKm*t©•gPm⅔ê*kQB½-W_ñH”.er’07Hu—3ﬂ“.xB-aKﬂuKNﬂKqnKUm🤔éHZﬂ“G’n7C🤔nqKKﬂJ🤔$ m2BöHp½H!*W½4tq8½:5#kKu-ó*⁦!⁩M&.―x_ éL|[HaöH“m’⅓H⅔umKw×0Kﬂ@’“%Um…|.míBJ‘’W–q­nL0🗣½NE―½⅔Pf­jﬂ-K8@.-[_3{3“Z(👻‘'{D“H’]Q7© bVﬂãI’mI’),S.ﬂ.sHaYmez%―t👻hu*KáVD-9iu(umK’*.u7'm’.—V#uHHOC)bH8d!p;1“fkWy%⁩·–•Pn1K’g[!”G9#;rm―dK•nH—it\n",
      "178374/178374 [==============================] - 203s 1ms/sample - loss: 0.0031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13d5fc3c8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S2-NN(v1)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
