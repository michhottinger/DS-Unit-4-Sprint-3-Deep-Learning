{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# TODO - Words, words, mere words, no matter from the heart.\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "r = requests.get(url)\n",
    "r.encoding = r.apparent_encoding\n",
    "data = r.text\n",
    "\n",
    "\n",
    "#r.encoding = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5740053"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "text = \" \".join(data)\n",
    "\n",
    "chars = list(set(text))\n",
    "\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 1148003\n"
     ]
    }
   ],
   "source": [
    "# Create the Sequence Data\n",
    "\n",
    "maxlen = 80\n",
    "step = 10\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 characters long\n",
    "next_chars = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences:', len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_cut = sequences[:100000]\n",
    "len(sequences_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify x & y\n",
    "\n",
    "x = np.zeros((len(sequences_cut), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences_cut), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences_cut):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 80, 107)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 107)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature=1.0)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 3.0426\n",
      "----- Generating text after Epoch: 0\n",
      " ---- Generating with seed: \" a l l e d   V i n c e n t i o ;   m y   d w e l l i n g   P i s a ; \n",
      " A n d  \"\n",
      " a l l e d   V i n c e n t i o ;   m y   d w e l l i n g   P i s a ; \n",
      " A n d  erevniedlittthoencey   s-oast fsIunmer.\n",
      "\n",
      "\n",
      "\n",
      ".  TUGhiun ns, \n",
      "\n",
      "  \n",
      " \n",
      "\n",
      "\n",
      "r\n",
      "\n",
      " A  v ers   \n",
      "\n",
      "f\n",
      "PVEARRooCHN RPMAN.U\n",
      " y h a t\n",
      " \n",
      "L\n",
      " 'R@TI.s\n",
      "\n",
      "O hEiirnig d  veaynt  oofsliunnt adsp eaetng en's s ,g\n",
      "\n",
      "\n",
      "    ;\n",
      "\n",
      " \n",
      "\n",
      "xx\n",
      "Tbulrreyrmeeyimlirs ,s\n",
      "\n",
      "\n",
      "\n",
      "OB\n",
      "\n",
      "AC II T E.B\n",
      "S\n",
      "F\n",
      "y  IC\n",
      "\n",
      "\n",
      "\n",
      "n II CAA\n",
      "I\n",
      "\n",
      "H.\n",
      " O oDr n-tthyi r ri kelte ept  t jhag iy  teletsl.r\n",
      "\n",
      "\n",
      "    hR o.A\n",
      "\n",
      "_F Amwoinasth ey storl\n",
      "100000/100000 [==============================] - 182s 2ms/sample - loss: 3.0423\n",
      "Epoch 2/8\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 2.3949\n",
      "----- Generating text after Epoch: 1\n",
      " ---- Generating with seed: \"r   V a r r o   a n d   C l a u d i u s . \n",
      " \n",
      " V A R R O . \n",
      " C a l l s   m \"\n",
      "    V a r r o   a n d   C l a u d i u s . \n",
      " \n",
      " V A R R O . \n",
      " C a l l s   m ipofng; \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "P\n",
      "\n",
      "Y\n",
      "   y  oud e\n",
      "\n",
      "    rey vier   dhe;   -oung ey\n",
      "l  ;r yot   t\n",
      "\n",
      "  it hea rne ,  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " n oer r;\n",
      "\n",
      " \n",
      "E\n",
      "EgS.   ,\n",
      "\n",
      " \n",
      " NIRPCEAI.\n",
      "WSlirsr .e  otnu sse,.,\n",
      "\n",
      "\n",
      "\n",
      "BT heayne,  \n",
      "\n",
      " \n",
      "Y   EWOARMO.DLBSI..  HFer.  KaDyce a kge  its   nIo udgrs.  \n",
      "\n",
      " \n",
      "b   YBeeyn  \n",
      "'AIur’dg g\n",
      "\n",
      "h\n",
      "p eTrher es  oti,s ,s  t\n",
      "oourrte rsel Ps .   Yolourrd  ekl .i\n",
      "\n",
      "   FOONNODAR..Ig\n",
      "100000/100000 [==============================] - 178s 2ms/sample - loss: 2.3948\n",
      "Epoch 3/8\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 2.1889\n",
      "----- Generating text after Epoch: 2\n",
      " ---- Generating with seed: \"n ,   h i s   e y e s   b e g u n \n",
      " T o   w i n k ,   b e i n g   b l i n d e \"\n",
      "  ,   h i s   e y e s   b e g u n \n",
      " T o   w i n k ,   b e i n g   b l i n d e sr.\n",
      "vWhienlceed  asn t \n",
      "\n",
      "     LOAVN‘AAR..  EgThaitst  'y  . \n",
      "\n",
      "\n",
      "\n",
      "B EFSRPUDN..\n",
      "\n",
      "z Ihry;  ss,   Toou  rgt  \n",
      "FYoirsg: \n",
      "W indv.i\n",
      "\n",
      "  Brinst   fToou   atr es .,\n",
      "\n",
      "\n",
      " \n",
      "KSOhiy.y\n",
      "\n",
      "  IEPR.R\n",
      " SD,E  SLOIeRD.e\n",
      "WIor . \n",
      "\n",
      "\n",
      "BB[eost  th’een \n",
      "\n",
      "I   slte  miyn \n",
      "\n",
      " \n",
      "P\n",
      "\n",
      "I M )itr. \n",
      "\n",
      "\n",
      "\n",
      "YCTey.\n",
      "\n",
      " d  chy',   thael e\n",
      "dJou rn  oum  ssee  ste ’ y  OC]NBOLPANR.T  RDoS. \n",
      "\n",
      "IA[aEndo s \n",
      "D Ds“\n",
      "100000/100000 [==============================] - 249s 2ms/sample - loss: 2.1889\n",
      "Epoch 4/8\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 2.0773\n",
      "----- Generating text after Epoch: 3\n",
      " ---- Generating with seed: \" d . \n",
      "         A h ,   m y   g o o d   l o r d ,   I   g r i e v e   a t   w h\"\n",
      " d . \n",
      "         A h ,   m y   g o o d   l o r d ,   I   g r i e v e   a t   w haant ed  dnye. \n",
      "\n",
      "\n",
      "\n",
      "H\n",
      "SaOI s ,t myan't s. \n",
      "\n",
      "    CMxyeurtt s?\n",
      "\n",
      "F\n",
      "yEUNYGA. _\n",
      "H\n",
      "\n",
      "  Nftiirnt \n",
      "\n",
      "    t  rerne  thou rge  dlieakuer,  mirlrt.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BIIRRtSR..\n",
      "\n",
      "\n",
      "s hreye  emn  dd eem\n",
      " oo t;  fluye de\n",
      "nnooo  ndt ,m end  y'agr yo uvee. \n",
      "\n",
      "sAIn\n",
      "\n",
      "\n",
      "n\n",
      "; oours  tanl or\n",
      " \n",
      "APThheirs  eyw,  imilbleiss  ennt  nooft  trueess m'e, \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "T\n",
      "!PRhoi r\n",
      " T ha ys\n",
      "\n",
      "w storr   otree  .\n",
      "100000/100000 [==============================] - 199s 2ms/sample - loss: 2.0773\n",
      "Epoch 5/8\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.9974\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \" r   l i k e   m i c e ;   a n d   y o n d   t a l l   a n c h o r i n g   b a r\"\n",
      "sr   l i k e   m i c e ;   a n d   y o n d   t a l l   a n c h o r i n g   b a rste'ncde  wheye; \n",
      "i;n der  geet; \n",
      "\n",
      "    NCO7xLOKAP..\n",
      "\n",
      "PHourrierler.  \n",
      "\n",
      "\n",
      "\n",
      "BHiowny,.\n",
      "\n",
      "    HIxEPRAf.  miep’n? \n",
      "\n",
      "I\n",
      "_\n",
      "S\n",
      "I\n",
      "\n",
      "s IoV',n  stterirnc;eamra.r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "F htaintt  .w\n",
      "\n",
      "\n",
      "I RpI._\n",
      "\n",
      "\n",
      "\n",
      "dI rwielndcte e;  ,n oung 'dakIn isc esd,  btueinlgt eerss  .c\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ILON_.\n",
      "\n",
      "nd . \n",
      "\n",
      "_E ndo ’tt  'msd  ytnuovueym.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tIUENNNY ..  _Mer.s\n",
      "g ancde  een , \n",
      " O Jyien ;\n",
      "100000/100000 [==============================] - 249s 2ms/sample - loss: 1.9974\n",
      "Epoch 6/8\n",
      " 99968/100000 [============================>.] - ETA: 0s - loss: 1.9367\n",
      "----- Generating text after Epoch: 5\n",
      " ---- Generating with seed: \" m e r ,   a n d   b y   t h e e , \n",
      " B e   s t i l ' d   t h e   L o r d   o '\"\n",
      " m e r ,   a n d   b y   t h e e , \n",
      " B e   s t i l ' d   t h e   L o r d   o '\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PAILC_OESN.\n",
      "\n",
      "\n",
      "\n",
      "E\n",
      "vCIur\n",
      "s\n",
      "TCoissn' cleee. \n",
      "\n",
      " TAonss,DSYouoorwt fkendt ,  hiei.n  sthoey , \n",
      "\n",
      "    h emu.  ID y. \n",
      "\n",
      " Biast eye.'  sh’eard \n",
      " Treeeirdme ,\n",
      " Gouunrked .y\n",
      "     ARNTOANN..\n",
      "\n",
      "LIEyc. \n",
      "\n",
      " T]p,\n",
      "wWheiisvtu yd. \n",
      "\n",
      "BBeeert;  onrgiyu rnte rts..\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "_BIA LRLAE..\n",
      "\n",
      "\n",
      "S ,b es. \n",
      "\n",
      " Iuirltde,,\n",
      "nIovte,. \n",
      "\n",
      "F\n",
      "PBEEROONM..\n",
      "\n",
      "IY.  ;S ey  Rnoo twe'.d\n",
      "\n",
      "n\n",
      "BLyouutrs .s\n",
      "100000/100000 [==============================] - 233s 2ms/sample - loss: 1.9367\n",
      "Epoch 7/8\n",
      " 94976/100000 [===========================>..] - ETA: 5:29 - loss: 1.8850\n",
      "----- Generating text after Epoch: 6\n",
      " ---- Generating with seed: \" t . \n",
      " \n",
      " E G E O N . \n",
      " I s   n o t   y o u r   n a m e ,   s i r ,   c a l\"\n",
      " t . \n",
      " \n",
      " E G E O N . \n",
      "DI s   n o t   y o u r   n a m e ,   s i r ,   c a lkleevle, \n",
      "\n",
      "onrneorr;  wwoilllle-. \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I G'lyE.\n",
      "\n",
      "NIiesnrre. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PAICDLNGAE..\n",
      "\n",
      " Nnuout,  wfierxvteeerl’,\n",
      "mmuyr toirguceed, \n",
      "\n",
      "v\n",
      "\n",
      "  RRAEUNND..\n",
      "\n",
      "T\n",
      "HAyo,s  sfliintgtge.  ,\n",
      "\n",
      "\n",
      "\n",
      "BFCLEINFG.\n",
      "\n",
      "\n",
      " Thuirl e , \n",
      "\n",
      "\n",
      "\n",
      "BIruet  \n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "\n",
      "KELGU..\n",
      "\n",
      "nBTeadeee  htire sdrererei nng' rt  inngd ,u sgtt ,y]o\n",
      " \n",
      "\n",
      "ICr eye .\n",
      "\n",
      "\n",
      "\n",
      "_IIN].\n",
      "NYioat,\n",
      "\n",
      "WOirde’ ss;  hs’  sthyienls,  s\n",
      "100000/100000 [==============================] - 6142s 61ms/sample - loss: 1.8842\n",
      "Epoch 8/8\n",
      " 91008/100000 [==========================>...] - ETA: 7:33 - loss: 1.8450\n",
      "----- Generating text after Epoch: 7\n",
      " ---- Generating with seed: \" r o m   d a y   t o   d a y \n",
      "         V i s i t   t h e   s p e e c h l e s s\"\n",
      " r o m   d a y   t o   d a y \n",
      "p        V i s i t   t h e   s p e e c h l e s se,  ’tnaedv’ess \n",
      "soou’t. \n",
      "\n",
      " \n",
      "E\n",
      "N EEENNTN..\n",
      "\n",
      "IN t  atn'et .\n",
      "\n",
      "    LOyOUNP.  HHee. \n",
      "\n",
      "B uesrt  hooun  tihc ndirvee’nd.   Doe’t’  dseet . \n",
      "\n",
      "i\n",
      "F ELyCEIUND..\n",
      "\n",
      "NIUrtt. \n",
      "\n",
      "    CCAeEANDA.’\n",
      "\n",
      "MEednginngtese iesn . \n",
      "\n",
      " \n",
      "\n",
      "LLEyENE..\n",
      "\n",
      "M i’sssee  ,v eadryi,s  breiytr,  \n",
      "\n",
      ",Farliaetl \n",
      "\n",
      "I   mMy.,  BMueiokt  htou;  gihtn.y  ttheiangc.t  sutearreisg.y\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "L LIEGG..\n",
      "\n",
      "s  theina f'te\n",
      "100000/100000 [==============================] - 4509s 45ms/sample - loss: 1.8422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7906ea7b8>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=8,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NN(v1)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
